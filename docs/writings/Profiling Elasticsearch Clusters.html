<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Profiling Elasticsearch Clusters</title>
    <link rel="stylesheet" type="text/css" media="screen" href="https://seanngpengnam.com/normalize.css">
    <!--
    <link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:500,100,300" rel="stylesheet">
    -->
    <link rel="stylesheet" type="text/css" media="screen" href="https://seanngpengnam.com/site.css">
  </head>

  <body>
      <div class="container wrapper post">
      <div class="header">
          <nav class="nav">
              <ul class="flat">
                <li> <a href="index.html">Writings Home</a></li>
              </ul>
          </nav>

    </div>


    <div class="post-header">
        <h1 class="title">Profiling Elasticsearch Clusters</h1>
        <div class="meta">Updated at: 08 Dec 2019 14:50:09</div>
    </div>
    <div class="markdown">

          

<h2>Background:</h2>

<p>I spent some time in Shopee (Southeast Asia&rsquo;s largest ecommerce platform) over summer of 2019 and was interning in the search team. One of my projects is to profile elasticsearch clusters (and design them).</p>

<p>This guide aims to come up with a mental framework to approach cluster design.</p>

<h2>Scope</h2>

<p>This guide assumes the user to have at least a basic understanding of: elasticsearch (ES), ES partitions, ES cross-cluster search, and the components of a single ES search request (scattering/gathering).</p>

<p>This project does not include ideas of adjusting heap size/file cache size) as from our garbage collection profiles we do not appear to have this problem. It is a separate problem that can be analysed independently from this tuning.</p>

<p><a href="https://www.elastic.co/blog/a-heap-of-trouble">https://www.elastic.co/blog/a-heap-of-trouble</a> is the best guide for memory size tuning.</p>

<h2>Defining Performance</h2>

<p>The goal of cluster design is to reconfigure the cluster to improve its performance. We decided that the following metrics are good indicators of performance because they are easier to measure, and are generally agreed upon.
We can separate performance to CPU based performance and Off-CPU based performance. The fact is that a gain in one type of performance will lead to a decline in the other, hence the goal would be to determine the sweet spot that gives a balance of both.</p>

<p>The performance based metrics that a cluster are evaluated on are [in brackets are the type of performance that the metric measures]:
CPU % on each node (&lt; 80% is ideal) [CPU based performance]
A higher CPU% is an indication of higher unreliability.
Search rate per node (lower is generally better)[Off-CPU based performance]
Here, off-CPU performance strongly correlates with the search rate of the cluster. Minimizing search rate per node generally decreases off-CPU time and also decreases time spent “scattering” and “gathering”.
Search latency of requests (lower the better) [Overall performance]
A lower latency is better.
Thread pool rejections (up till 100 is roughly acceptable) [Overall performance]
Thread pool rejections is an indication whether the capacity of processing on a single node is reaching its limits.
The QPS the cluster can handle before suffering a distinct decline in the performance of above [Overall performance]</p>

<p>Note that most of these metrics have similar contributing factors. (For example, CPU and search latency).</p>

<p>These metrics should be evaluated on two types of requests: search requests and indexing requests. Search requests are more important than indexing requests as they involve user experience directly.
Profiling
The first step would be to profile your requests. By understanding the type and nature of the requests, we can understand the general load profile. An example of the importance of this is the general increase in QPS limit from 17000 to 21000 by utilizing the _routing features of cross-cluster.
Make sure to get real, actual requests.
Parameters:</p>

<p>Fixed Parameters
Having 1 machine per shard (regardless of whether its a master or a replica) will help balance the workload of the search and indexing requests.</p>

<p>Main parameters to tune:
Increasing clusters (cross cluster search).
An increased number of data clusters can be seen as a method to cope with the size of data. (increase number of clusters, decrease data size on each cluster)
With a decrease in the size of data of each individual cluster, then there is a better likelihood that performance can be better managed.
Increasing/decreasing shards
Increasing number of shards will increase throughput (and latency) up to a point before decreasing
Increasing number of shards will decrease shard size, allowing processing to be distributed across more machine nodes
But increasing number of shards will result in more cpu and off-cpu time for “scattering” and “gathering” phases
Increasing/decreasing replicas [least important]
Increasing number of replicas will decrease throughput (and latency) of search requests
Increasing number of replicas will increase throughput (and latency) of indexing requests</p>


    </div>
</div>
    <div class="footer">
        by <a href="https://seanngpengnam.com">Sean Ng</a> | <a href="https://github.com/pengnam/BrainDump">source</a>
  </div>
  </body>
</html>
